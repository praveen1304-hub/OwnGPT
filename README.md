
________________________________________
ğŸ¤– Local GPT Chatbot using Ollama + Streamlit
This is a personal chatbot project that runs a local LLM (Large Language Model) using Ollama and provides a simple web interface through Streamlit.
ğŸ’¬ Ask anything â€” the chatbot responds using the locally hosted qwen2.5:0.5b model or any other supported Ollama model.
________________________________________
âœ¨ Features
â€¢	ğŸ§  Chatbot powered by a local LLM (via Ollama)
â€¢	ğŸ” Maintains chat history within the session
â€¢	âœï¸ Shows typing status while processing
â€¢	ğŸ§¹ "Clear Chat" button to reset the session
________________________________________
ğŸ—‚ Project Structure
.
â”œâ”€â”€ app.py            # Streamlit interface for the chatbot
â”œâ”€â”€ callollama.py     # Logic to call the Ollama LLM API
________________________________________
ğŸ›  Requirements
â€¢	Python 3.8 or higher
â€¢	Ollama installed and running locally
â€¢	One or more Ollama-compatible models (e.g., qwen2.5:0.5b)
Install Python dependencies:
pip install streamlit requests
________________________________________
ğŸ“¦ Install and Run Ollama
ollama run qwen2.5:0.5b
This downloads and launches the qwen2.5:0.5b model on http://localhost:11434.
________________________________________
â–¶ï¸ Running the Chatbot
streamlit run app.py
Then go to your browser and open http://localhost:8501.
________________________________________
ğŸ“‹ How It Works
â€¢	app.py creates a Streamlit UI that displays messages and handles user input.
â€¢	callollama.py sends prompts to the local Ollama API and retrieves generated responses from the selected LLM.
________________________________________
ğŸ§ª Example Usage
Type something like:
What's the capital of Japan?
Or:
Explain Python decorators with examples.
________________________________________
ğŸ§  Model Customization
You can switch models by editing this line in callollama.py:
"model": "qwen2.5:0.5b"
Change to any other available model in Ollama like llama3, mistral, etc.
________________________________________
ğŸ“„ License
This project is open-source under the MIT License.
________________________________________

